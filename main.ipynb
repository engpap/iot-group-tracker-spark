{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/28 18:07:09 WARN Utils: Your hostname, Andrews-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.6.209.31 instead (on interface en0)\n",
      "24/04/28 18:07:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/28 18:07:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.6.209.31:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>data</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x147479f10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/28 18:07:24 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('data').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------------+--------------------+--------------------+\n",
      "|group_id|deviceIDs_list|          created_at|          updated_at|          deleted_at|\n",
      "+--------+--------------+--------------------+--------------------+--------------------+\n",
      "|       1|       [1,2,3]|2022-07-14 21:14:...|                null|                null|\n",
      "|       1|     [1,2,3,4]|2022-07-14 21:14:...|2022-07-15 21:14:...|                null|\n",
      "|       1|   [1,2,3,4,5]|2022-07-14 21:14:...|2022-07-17 21:14:...|2022-07-18 21:14:...|\n",
      "|       2|       [1,2,3]|2022-08-14 21:14:...|                null|                null|\n",
      "|       2|     [1,2,3,4]|2022-08-14 21:14:...|2022-08-15 21:14:...|                null|\n",
      "|       3|       [1,2,3]|2022-09-14 21:14:...|                null|                null|\n",
      "|       3|     [1,2,3,4]|2022-09-14 21:14:...|2022-09-15 21:14:...|                null|\n",
      "+--------+--------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark = spark.read.csv('groups.csv')\n",
    "# change column names\n",
    "df_spark = spark.read.option('header', 'true').csv('groups.csv')\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lifetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------------+--------------------+--------------------+\n",
      "|group_id|deviceIDs_list|          created_at|          updated_at|          deleted_at|\n",
      "+--------+--------------+--------------------+--------------------+--------------------+\n",
      "|       1|       [1,2,3]|2022-07-14 21:14:...|                null|                NULL|\n",
      "|       1|     [1,2,3,4]|2022-07-14 21:14:...|2022-07-15 21:14:...|                NULL|\n",
      "|       1|   [1,2,3,4,5]|2022-07-14 21:14:...|2022-07-17 21:14:...|2022-07-18 21:14:...|\n",
      "|       2|       [1,2,3]|2022-08-14 21:14:...|                null|                NULL|\n",
      "|       2|     [1,2,3,4]|2022-08-14 21:14:...|2022-08-15 21:14:...|                NULL|\n",
      "|       3|       [1,2,3]|2022-09-14 21:14:...|                null|                NULL|\n",
      "|       3|     [1,2,3,4]|2022-09-14 21:14:...|2022-09-15 21:14:...|                NULL|\n",
      "+--------+--------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "+--------+--------------+--------------------+--------------------+--------------------+\n",
      "|group_id|deviceIDs_list|          created_at|          updated_at|          deleted_at|\n",
      "+--------+--------------+--------------------+--------------------+--------------------+\n",
      "|       1|       [1,2,3]|2022-07-14 21:14:...|                null|2024-04-28 19:26:...|\n",
      "|       1|     [1,2,3,4]|2022-07-14 21:14:...|2022-07-15 21:14:...|2024-04-28 19:26:...|\n",
      "|       1|   [1,2,3,4,5]|2022-07-14 21:14:...|2022-07-17 21:14:...|2022-07-18 21:14:...|\n",
      "|       2|       [1,2,3]|2022-08-14 21:14:...|                null|2024-04-28 19:26:...|\n",
      "|       2|     [1,2,3,4]|2022-08-14 21:14:...|2022-08-15 21:14:...|2024-04-28 19:26:...|\n",
      "|       3|       [1,2,3]|2022-09-14 21:14:...|                null|2024-04-28 19:26:...|\n",
      "|       3|     [1,2,3,4]|2022-09-14 21:14:...|2022-09-15 21:14:...|2024-04-28 19:26:...|\n",
      "+--------+--------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "+--------+--------------+--------------------+--------------------+--------------------+--------+\n",
      "|group_id|deviceIDs_list|          created_at|          updated_at|          deleted_at|lifetime|\n",
      "+--------+--------------+--------------------+--------------------+--------------------+--------+\n",
      "|       1|       [1,2,3]|2022-07-14 21:14:...|                null|2024-04-28 19:26:...|     654|\n",
      "|       1|     [1,2,3,4]|2022-07-14 21:14:...|2022-07-15 21:14:...|2024-04-28 19:26:...|     654|\n",
      "|       1|   [1,2,3,4,5]|2022-07-14 21:14:...|2022-07-17 21:14:...|2022-07-18 21:14:...|       4|\n",
      "|       2|       [1,2,3]|2022-08-14 21:14:...|                null|2024-04-28 19:26:...|     623|\n",
      "|       2|     [1,2,3,4]|2022-08-14 21:14:...|2022-08-15 21:14:...|2024-04-28 19:26:...|     623|\n",
      "|       3|       [1,2,3]|2022-09-14 21:14:...|                null|2024-04-28 19:26:...|     592|\n",
      "|       3|     [1,2,3,4]|2022-09-14 21:14:...|2022-09-15 21:14:...|2024-04-28 19:26:...|     592|\n",
      "+--------+--------------+--------------------+--------------------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the lifetime of each group by substracting the timestamp of 'created_at' from the timestamp of 'deleted_at'\n",
    "# If 'deleted_at' is null, then the group is still active, so we will use the current timestamp\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_spark = df_spark.withColumn('created_at', col('created_at').cast('timestamp'))\n",
    "df_spark = df_spark.withColumn('deleted_at', col('deleted_at').cast('timestamp'))\n",
    "df_spark.show()\n",
    "\n",
    "# replace null values in 'deleted_at' with the current timestamp\n",
    "from pyspark.sql.functions import current_timestamp, coalesce\n",
    "df_spark_non_null = df_spark.withColumn('deleted_at', coalesce(col(\"deleted_at\"), current_timestamp()))\n",
    "df_spark_non_null.show()\n",
    "\n",
    "# if there's a timestamp in 'deleted_at', remove all the other rows with the same 'group_id'\n",
    "# TODO\n",
    "\n",
    "# if there are multiple rows with the same 'group_id' just keep the first one and remove the rest\n",
    "# for the lifetime calculation, it doesn't matter if we keep the first or the last row because \n",
    "# the lifetime is calculated by substracting the created_at from the now()\n",
    "# TODO\n",
    "\n",
    "\n",
    "# calculate the lifetime of each group\n",
    "from pyspark.sql.functions import datediff\n",
    "df_spark_lifetime = df_spark_non_null.withColumn('lifetime', datediff(col('deleted_at'), col('created_at')))\n",
    "df_spark_lifetime.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avg, Min, Max Cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|            group_id|avg(deviceIDs_list)|\n",
      "+--------------------+-------------------+\n",
      "|c15eb63e-bc0f-412...|               NULL|\n",
      "|160a2907-ce33-43a...|               NULL|\n",
      "|5809f334-5f9e-44b...|               NULL|\n",
      "|e959e56f-9df2-425...|               NULL|\n",
      "|a91a31cd-ce59-4b9...|               NULL|\n",
      "|d5405986-c54c-474...|               NULL|\n",
      "|2a8fd6e1-4fae-478...|               NULL|\n",
      "|b5e39e1e-ea8a-4da...|               NULL|\n",
      "|4b597716-2155-498...|               NULL|\n",
      "|9c1e473b-b2e7-45b...|               NULL|\n",
      "|4d69f5cf-8957-464...|               NULL|\n",
      "|cfba891c-256d-4f0...|               NULL|\n",
      "|0034ffcf-77d8-486...|               NULL|\n",
      "|3e5f13bb-0ea5-43f...|               NULL|\n",
      "|d668c66e-9d36-4c9...|               NULL|\n",
      "|f346d3f4-93b5-40e...|               NULL|\n",
      "|78b43764-17c9-48b...|               NULL|\n",
      "|93786375-d922-466...|               NULL|\n",
      "|8ba4b09f-d214-458...|               NULL|\n",
      "|e60c13cd-1f0d-4f8...|               NULL|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# idea: group by 'group_id' and calculate the average size of devicedIDs_list for each group \n",
    "from pyspark.sql.functions import avg\n",
    "df_spark_avg = df_spark_lifetime.groupBy('group_id').agg(avg('deviceIDs_list'))\n",
    "df_spark_avg.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
